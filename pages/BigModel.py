import streamlit as st
from langchain.schema import HumanMessage
from LLM.chat_glm4 import ChatGLM4
from LLM.chat_glm4v import ChatGLM4V
from LLM.cogview3 import ChatCogView3
from LLM.cogvideox import ChatCogVideoX

# åˆå§‹åŒ– LLM
llm_models = {
    "GLM-4 Flashï¼ˆæ–‡æœ¬ï¼‰": ChatGLM4(),
    "GLM-4V Flashï¼ˆå›¾åƒç†è§£ï¼‰": ChatGLM4V(),
    "CogView-3 Flashï¼ˆå›¾åƒç”Ÿæˆï¼‰": ChatCogView3(),
    "CogVideoX Flashï¼ˆè§†é¢‘ç”Ÿæˆï¼‰": ChatCogVideoX()
}

# Streamlit ç•Œé¢
st.title('ğŸ’¡ AI å¤šæ¨¡å‹ Chatbot')

# é€‰æ‹©å¤§æ¨¡å‹
model_choice = st.selectbox("è¯·é€‰æ‹©æ¨¡å‹ï¼š", list(llm_models.keys()))

# ä½¿ç”¨ session_state å­˜å‚¨å¯¹è¯å†å²
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# æ˜¾ç¤ºèŠå¤©å†å²è®°å½•
for chat in st.session_state.chat_history:
    with st.chat_message(chat["role"]):
        if "å›¾ç‰‡" in chat["type"]:  # âœ… å¦‚æœæ˜¯å›¾ç‰‡
            st.image(chat["content"], caption="AI ç”Ÿæˆçš„å›¾ç‰‡", use_container_width=True)  # âœ… ç§»é™¤ unsafe_allow_html
        elif "è§†é¢‘" in chat["type"]:  # âœ… å¦‚æœæ˜¯è§†é¢‘
            st.video(chat["content"])
        else:  # âœ… é»˜è®¤æ–‡æœ¬
            st.markdown(chat["content"])

# ç”¨æˆ·è¾“å…¥æ¡†
user_input = st.chat_input("è¯·è¾“å…¥ä½ çš„æ¶ˆæ¯..." if "æ–‡æœ¬" in model_choice else "è¯·ä¸Šä¼ å›¾ç‰‡æˆ–è¾“å…¥æè¿°")

if user_input:
    # è®°å½•ç”¨æˆ·è¾“å…¥
    st.session_state.chat_history.append({"role": "user", "content": user_input, "type": "text"})

    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(user_input)

    # è°ƒç”¨ LLM è·å– AI å“åº”
    messages = [HumanMessage(content=user_input)]
    selected_llm = llm_models[model_choice]

    with st.chat_message("assistant"):
        response_placeholder = st.empty()  # å ä½ç¬¦
        response_text = ""

        if isinstance(selected_llm, ChatGLM4) or isinstance(selected_llm, ChatGLM4V):
            # âœ… æ–‡æœ¬è¾“å‡º
            for chunk in selected_llm.stream(messages):
                if isinstance(chunk, str):
                    response_text += chunk
                else:
                    response_text += str(chunk)

                response_placeholder.write(response_text)  # å®æ—¶æ›´æ–°æ–‡æœ¬
            st.session_state.chat_history.append({"role": "assistant", "content": response_text, "type": "text"})

        elif isinstance(selected_llm, ChatCogView3):
            # âœ… ç”Ÿæˆå›¾ç‰‡
            image_url = selected_llm.invoke(user_input)
            st.image(image_url, caption="AI ç”Ÿæˆçš„å›¾ç‰‡", use_container_width=True)
            st.session_state.chat_history.append({"role": "assistant", "content": image_url, "type": "å›¾ç‰‡"})

        elif isinstance(selected_llm, ChatCogVideoX):
            # âœ… ç”Ÿæˆè§†é¢‘
            video_url = selected_llm.invoke(user_input)
            st.video(video_url)
            st.session_state.chat_history.append({"role": "assistant", "content": video_url, "type": "è§†é¢‘"})
