import streamlit as st
import streamlit.components.v1 as components
from langchain.schema import HumanMessage
from LLM.chat_glm4 import ChatGLM4

def main():
    st.title("ğŸ™ï¸ è¯­éŸ³è¯†åˆ«ä¸ ChatGLM4 å¯¹è¯")
    
    # åˆå§‹åŒ–æ¨¡å‹
    if 'chat_glm' not in st.session_state:
        st.session_state.chat_glm = ChatGLM4()
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # æ·»åŠ HTMLç»„ä»¶ç”¨äºè¯­éŸ³è¯†åˆ«
    components.html(
        """
        <div style="display: flex; align-items: center; gap: 10px;">
            <button id="startButton" onclick="startSpeechRecognition()" style="
                padding: 10px 20px;
                background-color: #ff4b4b;
                color: white;
                border: none;
                border-radius: 5px;
                cursor: pointer;
            ">
                ğŸ¤ å¼€å§‹å½•éŸ³
            </button>
            <div id="status" style="color: #666;"></div>
        </div>
        <div id="result" style="margin-top: 10px; color: #333;"></div>

        <script>
        let recognition;
        let isRecording = false;

        if ('webkitSpeechRecognition' in window) {
            recognition = new webkitSpeechRecognition();
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'zh-CN';  // è®¾ç½®ä¸ºä¸­æ–‡

            recognition.onstart = function() {
                document.getElementById('status').textContent = 'æ­£åœ¨å½•éŸ³...';
                document.getElementById('startButton').textContent = 'ğŸ›‘ åœæ­¢å½•éŸ³';
                document.getElementById('startButton').style.backgroundColor = '#4CAF50';
                isRecording = true;
            };

            recognition.onend = function() {
                document.getElementById('status').textContent = 'å½•éŸ³å·²åœæ­¢';
                document.getElementById('startButton').textContent = 'ğŸ¤ å¼€å§‹å½•éŸ³';
                document.getElementById('startButton').style.backgroundColor = '#ff4b4b';
                isRecording = false;
            };

            recognition.onresult = function(event) {
                let final_transcript = '';
                let interim_transcript = '';

                for (let i = event.resultIndex; i < event.results.length; ++i) {
                    if (event.results[i].isFinal) {
                        final_transcript += event.results[i][0].transcript;
                        // å°†ç»“æœå‘é€åˆ°Streamlit
                        window.parent.postMessage({
                            type: 'speech_recognition',
                            text: final_transcript
                        }, '*');
                    } else {
                        interim_transcript += event.results[i][0].transcript;
                    }
                }

                document.getElementById('result').innerHTML = 
                    (interim_transcript ? 'ä¸´æ—¶ç»“æœ: ' + interim_transcript : '') +
                    (final_transcript ? '<br>æœ€ç»ˆç»“æœ: ' + final_transcript : '');
            };

            recognition.onerror = function(event) {
                document.getElementById('status').textContent = 'é”™è¯¯: ' + event.error;
            };
        }

        function startSpeechRecognition() {
            if (!recognition) {
                alert('æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½');
                return;
            }

            if (isRecording) {
                recognition.stop();
            } else {
                recognition.start();
            }
        }
        </script>
        """,
        height=150,
    )

    # æ–‡æœ¬è¾“å…¥æ¡†
    user_input = st.chat_input("è¾“å…¥æ–‡å­—æˆ–ä½¿ç”¨è¯­éŸ³...")

    # å¤„ç†æ–‡æœ¬è¾“å…¥
    if user_input:
        process_input(user_input)

    # æ˜¾ç¤ºèŠå¤©å†å²
    for chat in st.session_state.chat_history:
        with st.chat_message(chat["role"]):
            st.markdown(chat["content"])

def process_input(text):
    """å¤„ç†è¾“å…¥æ–‡æœ¬å¹¶è·å–æ¨¡å‹å“åº”"""
    if not text.strip():
        return
        
    # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°å†å²è®°å½•
    st.session_state.chat_history.append({
        "role": "user",
        "content": text
    })
    
    # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    with st.chat_message("user"):
        st.markdown(text)

    # è·å–æ¨¡å‹å“åº”
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        response_text = ""
        
        # ä½¿ç”¨ ChatGLM4 çš„æµå¼å“åº”
        for chunk in st.session_state.chat_glm.stream([HumanMessage(content=text)]):
            response_text += str(chunk)
            response_placeholder.markdown(response_text)
        
        # æ·»åŠ åŠ©æ‰‹å“åº”åˆ°å†å²è®°å½•
        st.session_state.chat_history.append({
            "role": "assistant",
            "content": response_text
        })

# æ·»åŠ JavaScriptå›è°ƒå¤„ç†å™¨
def handle_speech_input():
    components.html(
        """
        <script>
        window.addEventListener('message', function(e) {
            if (e.data.type === 'speech_recognition') {
                // å°†è¯†åˆ«ç»“æœå‘é€åˆ°Streamlitåç«¯
                window.parent.Streamlit.setComponentValue(e.data.text);
            }
        });
        </script>
        """,
        height=0,
    )

if __name__ == "__main__":
    main()
    handle_speech_input()